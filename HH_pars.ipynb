{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbdbc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Unified links collector for hh.ru\n",
    "\n",
    "- STEP 1: first_pass_collect_links()  – initial collection, from page 0\n",
    "- STEP 1b: second_pass_extend_links() – extend existing queries, with control of start page\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib.parse import quote_plus, urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# =====================================================================\n",
    "# CONFIG: MAIN QUERY LIST (used for FIRST PASS)\n",
    "# =====================================================================\n",
    "\n",
    "QUERIES = [\n",
    "    # Python разработка\n",
    "    \"Python разработчик\",\n",
    "    \"Python developer\",\n",
    "    \"Backend Python\",\n",
    "    \"Fullstack Python\",\n",
    "    \"Django разработчик\",\n",
    "    \"Flask разработчик\",\n",
    "    \"FastAPI разработчик\",\n",
    "\n",
    "    # Frontend разработка\n",
    "    \"Frontend разработчик\",\n",
    "    \"React разработчик\",\n",
    "    \"Vue разработчик\",\n",
    "    \"Angular разработчик\",\n",
    "    \"JavaScript разработчик\",\n",
    "    \"TypeScript разработчик\",\n",
    "    \"HTML/CSS верстальщик\",\n",
    "\n",
    "    # Mobile разработка\n",
    "    \"iOS разработчик\",\n",
    "    \"Android разработчик\",\n",
    "    \"React Native разработчик\",\n",
    "    \"Flutter разработчик\",\n",
    "    \"Mobile developer\",\n",
    "\n",
    "    # Backend разработка (другие языки)\n",
    "    \"Java разработчик\",\n",
    "    \"C# разработчик\",\n",
    "    \".NET разработчик\",\n",
    "    \"PHP разработчик\",\n",
    "    \"Go разработчик\",\n",
    "    \"Golang разработчик\",\n",
    "    \"Node.js разработчик\",\n",
    "    \"Ruby разработчик\",\n",
    "    \"Scala разработчик\",\n",
    "\n",
    "    # Data Science & ML\n",
    "    \"Data Scientist\",\n",
    "    \"Machine Learning Engineer\",\n",
    "    \"ML Engineer\",\n",
    "    \"Deep Learning Engineer\",\n",
    "    \"AI Engineer\",\n",
    "    \"Computer Vision Engineer\",\n",
    "    \"NLP Engineer\",\n",
    "    \"MLOps Engineer\",\n",
    "\n",
    "    # Data Engineering & Analytics\n",
    "    \"Data Engineer\",\n",
    "    \"Data Analyst\",\n",
    "    \"Аналитик данных\",\n",
    "    \"BI Analyst\",\n",
    "    \"Business Intelligence\",\n",
    "    \"Big Data Engineer\",\n",
    "    \"ETL разработчик\",\n",
    "\n",
    "    # DevOps & Infrastructure\n",
    "    \"DevOps Engineer\",\n",
    "    \"SRE\",\n",
    "    \"Site Reliability Engineer\",\n",
    "    \"Cloud Engineer\",\n",
    "    \"Kubernetes Engineer\",\n",
    "    \"System Administrator\",\n",
    "    \"Infrastructure Engineer\",\n",
    "\n",
    "    # QA & Testing\n",
    "    \"QA Engineer\",\n",
    "    \"Тестировщик\",\n",
    "    \"Automation QA\",\n",
    "    \"Manual QA\",\n",
    "    \"Performance Engineer\",\n",
    "    \"Test Engineer\",\n",
    "\n",
    "    # Security\n",
    "    \"Information Security\",\n",
    "    \"Security Engineer\",\n",
    "    \"Penetration Tester\",\n",
    "    \"Security Analyst\",\n",
    "    \"Cybersecurity\",\n",
    "\n",
    "    # Product & Project Management\n",
    "    \"Product Manager\",\n",
    "    \"Project Manager IT\",\n",
    "    \"Technical Project Manager\",\n",
    "    \"Product Owner\",\n",
    "    \"Scrum Master\",\n",
    "    \"Agile Coach\",\n",
    "\n",
    "    # Design\n",
    "    \"UI/UX Designer\",\n",
    "    \"Product Designer\",\n",
    "    \"Graphic Designer IT\",\n",
    "    \"Motion Designer\",\n",
    "\n",
    "    # Architecture & Leadership\n",
    "    \"Solution Architect\",\n",
    "    \"Software Architect\",\n",
    "    \"Technical Lead\",\n",
    "    \"Team Lead\",\n",
    "    \"CTO\",\n",
    "    \"Head of Engineering\",\n",
    "\n",
    "    # Game Development\n",
    "    \"Game Developer\",\n",
    "    \"Unity Developer\",\n",
    "    \"Unreal Engine Developer\",\n",
    "    \"Game Designer\",\n",
    "\n",
    "    # Специализированные роли\n",
    "    \"Blockchain Developer\",\n",
    "    \"Smart Contract Developer\",\n",
    "    \"Embedded Developer\",\n",
    "    \"IoT Developer\",\n",
    "    \"Firmware Engineer\",\n",
    "\n",
    "    # Junior позиции\n",
    "    \"Junior Python\",\n",
    "    \"Junior Java\",\n",
    "    \"Junior Frontend\",\n",
    "    \"Junior Backend\",\n",
    "    \"Junior Developer\",\n",
    "    \"Стажер программист\",\n",
    "]\n",
    "\n",
    "# =====================================================================\n",
    "# CONFIG: QUERIES FOR EXTENSION (used for SECOND PASS)\n",
    "# (this is your extended list from the old second cell)\n",
    "# =====================================================================\n",
    "\n",
    "EXISTING_QUERIES_TO_EXTEND = [\n",
    "    \"Python разработчик\",\n",
    "    \"Python developer\",\n",
    "    \"Backend Python\",\n",
    "    \"Fullstack Python\",\n",
    "    \"Django разработчик\",\n",
    "    \"Flask разработчик\",\n",
    "    \"FastAPI разработчик\",\n",
    "    \"Frontend разработчик\",\n",
    "    \"React разработчик\",\n",
    "    \"Vue разработчик\",\n",
    "    \"Angular разработчик\",\n",
    "    \"JavaScript разработчик\",\n",
    "    \"TypeScript разработчик\",\n",
    "    \"HTML/CSS верстальщик\",\n",
    "    \"iOS разработчик\",\n",
    "    \"Android разработчик\",\n",
    "    \"React Native разработчик\",\n",
    "    \"Flutter разработчик\",\n",
    "    \"Mobile developer\",\n",
    "    \"Java разработчик\",\n",
    "    \"C# разработчик\",\n",
    "    \".NET разработчик\",\n",
    "    \"PHP разработчик\",\n",
    "    \"Go разработчик\",\n",
    "    \"Golang разработчик\",\n",
    "    \"Node.js разработчик\",\n",
    "    \"Ruby разработчик\",\n",
    "    \"Scala разработчик\",\n",
    "    \"Data Scientist\",\n",
    "    \"Machine Learning Engineer\",\n",
    "    \"ML Engineer\",\n",
    "    \"Deep Learning Engineer\",\n",
    "    \"AI Engineer\",\n",
    "    \"Computer Vision Engineer\",\n",
    "    \"NLP Engineer\",\n",
    "    \"MLOps Engineer\",\n",
    "    \"Data Engineer\",\n",
    "    \"Data Analyst\",\n",
    "    \"Аналитик данных\",\n",
    "    \"BI Analyst\",\n",
    "    \"Business Intelligence\",\n",
    "    \"Big Data Engineer\",\n",
    "    \"ETL разработчик\",\n",
    "    \"DevOps Engineer\",\n",
    "    \"SRE\",\n",
    "    \"Site Reliability Engineer\",\n",
    "    \"Cloud Engineer\",\n",
    "    \"Kubernetes Engineer\",\n",
    "    \"System Administrator\",\n",
    "    \"Infrastructure Engineer\",\n",
    "    \"QA Engineer\",\n",
    "    \"Тестировщик\",\n",
    "    \"Automation QA\",\n",
    "    \"Manual QA\",\n",
    "    \"Performance Engineer\",\n",
    "    \"Test Engineer\",\n",
    "    \"Information Security\",\n",
    "    \"Security Engineer\",\n",
    "    \"Penetration Tester\",\n",
    "    \"Security Analyst\",\n",
    "    \"Cybersecurity\",\n",
    "    \"Product Manager\",\n",
    "    \"Project Manager IT\",\n",
    "    \"Technical Project Manager\",\n",
    "    \"Product Owner\",\n",
    "    \"Scrum Master\",\n",
    "    \"Agile Coach\",\n",
    "    \"UI/UX Designer\",\n",
    "    \"Product Designer\",\n",
    "    \"Graphic Designer IT\",\n",
    "    \"Motion Designer\",\n",
    "    \"Solution Architect\",\n",
    "    \"Software Architect\",\n",
    "    \"Technical Lead\",\n",
    "    \"Team Lead\",\n",
    "    \"CTO\",\n",
    "    \"Head of Engineering\",\n",
    "    \"Game Developer\",\n",
    "    \"Unity Developer\",\n",
    "    \"Unreal Engine Developer\",\n",
    "    \"Game Designer\",\n",
    "    \"Blockchain Developer\",\n",
    "    \"Smart Contract Developer\",\n",
    "    \"Embedded Developer\",\n",
    "    \"IoT Developer\",\n",
    "    \"Firmware Engineer\",\n",
    "    \"Junior Python\",\n",
    "    \"Junior Java\",\n",
    "    \"Junior Frontend\",\n",
    "    \"Junior Backend\",\n",
    "    \"Junior Developer\",\n",
    "    \"Стажер программист\",\n",
    "\n",
    "    # Дополнительные вариации из прежнего кода\n",
    "    \"Python backend\",\n",
    "    \"Python backend developer\",\n",
    "    \"Senior Python\",\n",
    "    \"Middle Python\",\n",
    "    \"Python engineer\",\n",
    "\n",
    "    \"JS разработчик\",\n",
    "    \"Frontend developer\",\n",
    "    \"Senior Frontend\",\n",
    "    \"Middle Frontend\",\n",
    "    \"Junior JavaScript\",\n",
    "    \"Верстальщик\",\n",
    "\n",
    "    \"Senior Java\",\n",
    "    \"Middle Java\",\n",
    "    \"Java backend\",\n",
    "    \"Java engineer\",\n",
    "\n",
    "    \"Мобильный разработчик\",\n",
    "    \"Senior iOS\",\n",
    "    \"Senior Android\",\n",
    "    \"Middle Android\",\n",
    "    \"Middle iOS\",\n",
    "\n",
    "    \"DevOps инженер\",\n",
    "    \"Senior DevOps\",\n",
    "    \"Middle DevOps\",\n",
    "    \"Linux администратор\",\n",
    "\n",
    "    \"Тестировщик ПО\",\n",
    "    \"Senior QA\",\n",
    "    \"Middle QA\",\n",
    "    \"Junior QA\",\n",
    "    \"Автоматизатор\",\n",
    "\n",
    "    \"Аналитик\",\n",
    "    \"Системный аналитик\",\n",
    "    \"Бизнес аналитик\",\n",
    "    \"Senior Data Scientist\",\n",
    "    \"Middle Data Scientist\",\n",
    "\n",
    "    \"C++ разработчик\",\n",
    "    \"Senior C++\",\n",
    "    \"PHP backend\",\n",
    "    \"Senior PHP\",\n",
    "\n",
    "    \"Fullstack разработчик\",\n",
    "    \"Fullstack developer\",\n",
    "    \"Full stack\",\n",
    "\n",
    "    \"1С программист\",\n",
    "    \"1С разработчик\",\n",
    "    \"Битрикс разработчик\",\n",
    "    \"Wordpress разработчик\",\n",
    "\n",
    "    \"UX дизайнер\",\n",
    "    \"UI дизайнер\",\n",
    "    \"Веб дизайнер\",\n",
    "]\n",
    "\n",
    "# =====================================================================\n",
    "# MAIN TUNABLE SETTINGS\n",
    "# =====================================================================\n",
    "\n",
    "# For first pass (you can override per-call)\n",
    "MAX_PAGES_PER_QUERY = 20   # ~ up to 2000 resumes per query\n",
    "\n",
    "# For extension pass\n",
    "PAGES_FOR_EXTEND = 50      # additional pages for existing professions\n",
    "\n",
    "# Smart stop\n",
    "STOP_AFTER_EMPTY_PAGES = 5  # stop after N pages in a row with < MIN_NEW_TO_CONTINUE new resumes\n",
    "MIN_NEW_TO_CONTINUE = 1     # if at least this many new resumes, continue\n",
    "\n",
    "PAUSE_BETWEEN_REQUESTS = 1.5\n",
    "\n",
    "LINKS_FILE = \"resumes_links.csv\"\n",
    "\n",
    "SEARCH_URL_TEMPLATE = (\n",
    "    \"https://hh.ru/search/resume\"\n",
    "    \"?text={query}\"\n",
    "    \"&from=suggest_post\"\n",
    "    \"&pos=full_text\"\n",
    "    \"&logic=normal\"\n",
    "    \"&exp_period=all_time\"\n",
    "    \"&ored_clusters=true\"\n",
    "    \"&order_by=relevance\"\n",
    "    \"&search_period=365\"\n",
    "    \"&items_on_page=100\"\n",
    "    \"&page={page}\"\n",
    ")\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/117.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.9\",\n",
    "}\n",
    "\n",
    "BASE_DOMAIN = \"https://hh.ru\"\n",
    "\n",
    "# =====================================================================\n",
    "# HELPERS\n",
    "# =====================================================================\n",
    "\n",
    "def ensure_dir_for_file(path: str) -> None:\n",
    "    d = os.path.dirname(os.path.abspath(path))\n",
    "    if d and not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "\n",
    "def append_row_to_csv(file_path: str, row: dict, header: list) -> None:\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    ensure_dir_for_file(file_path)\n",
    "    with open(file_path, \"a\", encoding=\"utf-8-sig\", newline=\"\") as fh:\n",
    "        writer = csv.DictWriter(fh, fieldnames=header)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "def resume_id_from_url(url: str) -> str | None:\n",
    "    if not url:\n",
    "        return None\n",
    "    if \"/resume/\" in url:\n",
    "        return url.split(\"/resume/\")[1].split(\"?\")[0].strip(\"/\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalize_tokens(query: str) -> list[str]:\n",
    "    return [\n",
    "        t.strip().lower()\n",
    "        for t in query.replace(\"-\", \" \").replace(\"/\", \" \").split()\n",
    "        if t.strip()\n",
    "    ]\n",
    "\n",
    "\n",
    "def load_existing_resume_ids(file_path: str) -> set[str]:\n",
    "    \"\"\"Load already collected resume_id values from CSV for duplicate checking.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return set()\n",
    "\n",
    "    existing_ids: set[str] = set()\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8-sig\") as fh:\n",
    "            reader = csv.DictReader(fh)\n",
    "            for row in reader:\n",
    "                rid = row.get(\"resume_id\")\n",
    "                if rid:\n",
    "                    existing_ids.add(rid)\n",
    "    except Exception as e:\n",
    "        print(f\"    [!] Error loading existing IDs: {e}\")\n",
    "\n",
    "    return existing_ids\n",
    "\n",
    "\n",
    "def get_query_page_count(file_path: str, query: str) -> int:\n",
    "    \"\"\"\n",
    "    Roughly estimate how many pages are ALREADY collected for a query,\n",
    "    based on the count of unique resume_id for this search_query.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return 0\n",
    "\n",
    "    query_ids: set[str] = set()\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8-sig\") as fh:\n",
    "            reader = csv.DictReader(fh)\n",
    "            for row in reader:\n",
    "                if row.get(\"search_query\") == query:\n",
    "                    rid = row.get(\"resume_id\")\n",
    "                    if rid:\n",
    "                        query_ids.add(rid)\n",
    "    except Exception as e:\n",
    "        print(f\"    [!] Error counting query pages: {e}\")\n",
    "        return 0\n",
    "\n",
    "    resume_count = len(query_ids)\n",
    "    if resume_count == 0:\n",
    "        return 0\n",
    "\n",
    "    # Conservative estimate: ~80 unique resumes per page\n",
    "    estimated_pages = (resume_count + 79) // 80\n",
    "    return estimated_pages\n",
    "\n",
    "# =====================================================================\n",
    "# CORE COLLECTOR\n",
    "# =====================================================================\n",
    "\n",
    "def collect_links_for_query(\n",
    "    query: str,\n",
    "    max_pages: int,\n",
    "    existing_ids: set[str],\n",
    "    start_page: int = 0,\n",
    "    stop_after_empty: int = STOP_AFTER_EMPTY_PAGES,\n",
    "    min_new_to_continue: int = MIN_NEW_TO_CONTINUE,\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Collect resume links for one search query.\n",
    "\n",
    "    Args:\n",
    "        query: search query string\n",
    "        max_pages: maximum number of pages to scan\n",
    "        existing_ids: set of already known resume_id (for duplicate skipping)\n",
    "        start_page: page to start from (0-based)\n",
    "        stop_after_empty: stop after N pages in a row with < min_new_to_continue new items\n",
    "        min_new_to_continue: minimum number of new resumes to consider a page \"non-empty\"\n",
    "    \"\"\"\n",
    "    tokens = normalize_tokens(query)\n",
    "    collected: list[dict] = []\n",
    "    new_ids_count = 0\n",
    "    empty_pages_streak = 0\n",
    "\n",
    "    print(f\"\\n>> Collecting links for query: {query!r}\")\n",
    "    print(f\"   Pages: {start_page} to {start_page + max_pages - 1}\")\n",
    "    print(\n",
    "        f\"   Stop rule: {stop_after_empty} empty pages in a row OR \"\n",
    "        f\"less than {min_new_to_continue} new\"\n",
    "    )\n",
    "\n",
    "    for page in range(start_page, start_page + max_pages):\n",
    "        url = SEARCH_URL_TEMPLATE.format(query=quote_plus(query), page=page)\n",
    "        print(f\"  [search] page {page}: {url}\")\n",
    "\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=15)\n",
    "            resp.raise_for_status()\n",
    "            html = resp.text\n",
    "        except Exception as e:\n",
    "            print(f\"    [!] request failed: {e}\")\n",
    "            break\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        anchors = soup.select('a[data-qa=\"serp-item__title\"]') or []\n",
    "\n",
    "        if not anchors:\n",
    "            containers = soup.select(\"div.resume-search-item, div.serp-item\")\n",
    "            anchors = []\n",
    "            for c in containers:\n",
    "                a = c.find(\"a\", href=lambda x: x and \"/resume/\" in x)\n",
    "                if a:\n",
    "                    anchors.append(a)\n",
    "\n",
    "        if not anchors:\n",
    "            all_links = soup.find_all(\"a\", href=lambda x: x and \"/resume/\" in x)\n",
    "            anchors = [\n",
    "                a\n",
    "                for a in all_links\n",
    "                if \"/resume/\" in a.get(\"href\", \"\")\n",
    "                and \"?\" not in a.get(\"href\", \"\").split(\"/resume/\")[1].split(\"?\")[0]\n",
    "            ]\n",
    "\n",
    "        if not anchors:\n",
    "            debug_fname = (\n",
    "                f\"debug_search_{query.replace(' ', '_').replace('/', '_')}_page{page}.html\"\n",
    "            )\n",
    "            with open(debug_fname, \"w\", encoding=\"utf-8\") as fh:\n",
    "                fh.write(html)\n",
    "            print(f\"    [!] No anchors found. Saved debug: {debug_fname}\")\n",
    "            # no anchors at all – likely end of results\n",
    "            break\n",
    "\n",
    "        page_links: list[dict] = []\n",
    "        page_new_count = 0\n",
    "        total_on_page = 0\n",
    "\n",
    "        for a in anchors:\n",
    "            href = a.get(\"href\")\n",
    "            if not href or \"/resume/\" not in href:\n",
    "                continue\n",
    "\n",
    "            href = urljoin(BASE_DOMAIN, href)\n",
    "            rid = resume_id_from_url(href)\n",
    "            if not rid:\n",
    "                continue\n",
    "\n",
    "            total_on_page += 1\n",
    "\n",
    "            # duplicate check\n",
    "            if rid in existing_ids:\n",
    "                continue\n",
    "\n",
    "            card_text = a.get_text(\" \", strip=True).lower()\n",
    "            if tokens and not any(tok in card_text for tok in tokens):\n",
    "                continue\n",
    "\n",
    "            title = a.get_text(strip=True) or card_text[:100]\n",
    "            obj = {\n",
    "                \"resume_id\": rid,\n",
    "                \"title\": title,\n",
    "                \"url\": href,\n",
    "                \"search_query\": query,\n",
    "                \"collected_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            }\n",
    "            page_links.append(obj)\n",
    "            existing_ids.add(rid)\n",
    "            page_new_count += 1\n",
    "\n",
    "        if page_new_count < min_new_to_continue:\n",
    "            empty_pages_streak += 1\n",
    "            print(\n",
    "                f\"    [!] Only {page_new_count} new (< {min_new_to_continue}), \"\n",
    "                f\"empty streak: {empty_pages_streak}/{stop_after_empty}\"\n",
    "            )\n",
    "        else:\n",
    "            empty_pages_streak = 0\n",
    "\n",
    "        print(\n",
    "            f\"    \\u2192 Page {page}: {page_new_count} NEW, \"\n",
    "            f\"{total_on_page - page_new_count} duplicates (total: {total_on_page}) \"\n",
    "            f\"| Total collected: {new_ids_count + page_new_count}\"\n",
    "        )\n",
    "\n",
    "        if page_links:\n",
    "            links_header = [\"resume_id\", \"title\", \"url\", \"search_query\", \"collected_at\"]\n",
    "            for row in page_links:\n",
    "                append_row_to_csv(LINKS_FILE, row, header=links_header)\n",
    "\n",
    "            collected.extend(page_links)\n",
    "            new_ids_count += page_new_count\n",
    "\n",
    "        # stop rules\n",
    "        if empty_pages_streak >= stop_after_empty:\n",
    "            print(\n",
    "                f\"    [STOP] {empty_pages_streak} pages with insufficient results, stopping\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "        if total_on_page == 0:\n",
    "            print(\"    [!] No results at all on page, likely end of results\")\n",
    "            break\n",
    "\n",
    "        time.sleep(PAUSE_BETWEEN_REQUESTS)\n",
    "\n",
    "    print(f\"  >> Total NEW links collected for '{query}': {new_ids_count}\")\n",
    "    return collected\n",
    "\n",
    "# =====================================================================\n",
    "# HIGH-LEVEL PASSES\n",
    "# =====================================================================\n",
    "\n",
    "def first_pass_collect_links(\n",
    "    queries=None,\n",
    "    max_pages_per_query: int = MAX_PAGES_PER_QUERY,\n",
    "    stop_after_empty: int = STOP_AFTER_EMPTY_PAGES,\n",
    "    min_new_to_continue: int = MIN_NEW_TO_CONTINUE,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    FIRST PASS:\n",
    "    - Uses QUERIES by default\n",
    "    - Always starts from page 0\n",
    "    - Works even if LINKS_FILE does not exist yet\n",
    "    \"\"\"\n",
    "    if queries is None:\n",
    "        queries = QUERIES\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"STEP 1: COLLECTING RESUME LINKS (FIRST PASS)\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    existing_ids = load_existing_resume_ids(LINKS_FILE)\n",
    "    print(f\">> Found {len(existing_ids)} existing resume IDs in {LINKS_FILE}\")\n",
    "    total_collected = 0\n",
    "\n",
    "    for idx, query in enumerate(queries, start=1):\n",
    "        print(f\"\\n[{idx}/{len(queries)}] Processing query: {query}\")\n",
    "        collected = collect_links_for_query(\n",
    "            query=query,\n",
    "            max_pages=max_pages_per_query,\n",
    "            existing_ids=existing_ids,\n",
    "            start_page=0,\n",
    "            stop_after_empty=stop_after_empty,\n",
    "            min_new_to_continue=min_new_to_continue,\n",
    "        )\n",
    "        total_collected += len(collected)\n",
    "        print(f\"  >> Collected {len(collected)} new links for this query\")\n",
    "        time.sleep(PAUSE_BETWEEN_REQUESTS)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\">> ALL DONE! Total NEW links collected: {total_collected}\")\n",
    "    print(f\">> Total unique resume IDs: {len(existing_ids)}\")\n",
    "    print(f\">> Links saved to: {LINKS_FILE}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def second_pass_extend_links(\n",
    "    queries=None,\n",
    "    pages_for_extend: int = PAGES_FOR_EXTEND,\n",
    "    start_page_override: int | None = None,\n",
    "    stop_after_empty: int = STOP_AFTER_EMPTY_PAGES,\n",
    "    min_new_to_continue: int = MIN_NEW_TO_CONTINUE,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    SECOND PASS:\n",
    "    - Extends existing queries (EXISTING_QUERIES_TO_EXTEND by default)\n",
    "    - If start_page_override is None → continue from last collected page (estimated)\n",
    "    - If start_page_override is set → start from that page number for ALL queries\n",
    "    \"\"\"\n",
    "    if queries is None:\n",
    "        queries = EXISTING_QUERIES_TO_EXTEND\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ADDING MORE QUERIES AND EXTENDING EXISTING ONES (SECOND PASS)\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nSettings:\")\n",
    "    print(f\"  - Extend existing: +{pages_for_extend} pages each\")\n",
    "    print(f\"  - Stop after: {stop_after_empty} empty pages in a row\")\n",
    "    print(f\"  - Continue if: at least {min_new_to_continue} new resume(s) found\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    existing_ids = load_existing_resume_ids(LINKS_FILE)\n",
    "    if not os.path.exists(LINKS_FILE):\n",
    "        print(f\"[!] Links file not found: {LINKS_FILE}\")\n",
    "        print(\"[!] No existing data – behaviour will be same as FIRST PASS from page 0.\")\n",
    "    print(f\"\\n>> Found {len(existing_ids)} existing resume IDs in {LINKS_FILE}\")\n",
    "    print(\">> This set will be used to skip duplicates during collection\\n\")\n",
    "\n",
    "    total_collected = 0\n",
    "    queries_processed = 0\n",
    "    queries_skipped = 0  \n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"PART 2: EXTENDING {len(queries)} EXISTING QUERIES\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for idx, query in enumerate(queries, start=1):\n",
    "        print(f\"\\n[EXTEND {idx}/{len(queries)}] Query: {query}\")\n",
    "\n",
    "        pages_done = get_query_page_count(LINKS_FILE, query)\n",
    "\n",
    "        if start_page_override is not None:\n",
    "            start_page = start_page_override\n",
    "            print(\n",
    "                f\"  Using user-specified start_page={start_page_override} \"\n",
    "                f\"(previous estimate pages_done={pages_done})\"\n",
    "            )\n",
    "        else:\n",
    "            if pages_done == 0:\n",
    "                print(\"  [!] No previous data found for this query. Starting from page 0.\")\n",
    "                start_page = 0\n",
    "            else:\n",
    "                start_page = pages_done\n",
    "                print(\n",
    "                    f\"  Already collected ~{pages_done} pages; \"\n",
    "                    f\"will continue from page {start_page}\"\n",
    "                )\n",
    "\n",
    "        collected = collect_links_for_query(\n",
    "            query=query,\n",
    "            max_pages=pages_for_extend,\n",
    "            existing_ids=existing_ids,\n",
    "            start_page=start_page,\n",
    "            stop_after_empty=stop_after_empty,\n",
    "            min_new_to_continue=min_new_to_continue,\n",
    "        )\n",
    "        total_collected += len(collected)\n",
    "        queries_processed += 1\n",
    "\n",
    "        if len(collected) > 0:\n",
    "            print(f\"  ✓ Collected {len(collected)} new links\")\n",
    "        else:\n",
    "            print(\"  ⚠ Collected 0 new links (all were duplicates or no results)\")\n",
    "\n",
    "        time.sleep(PAUSE_BETWEEN_REQUESTS * 2)\n",
    "\n",
    "    print(f\"\\n  Summary: Processed {queries_processed}, Skipped {queries_skipped}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ALL DONE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\">> Total NEW links collected this run: {total_collected}\")\n",
    "    print(f\">> Total unique resume IDs in database: {len(existing_ids)}\")\n",
    "    print(f\">> All links saved to: {LINKS_FILE}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"NEXT STEPS:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"1. Run your STEP 2 cell that parses resume details\")\n",
    "    print(\"   (it will automatically skip already parsed resumes)\")\n",
    "    print(\"2. Check resumes_details.csv for results\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "# =====================================================================\n",
    "# HOW TO RUN \n",
    "# =====================================================================\n",
    "# Just change MODE and re-run the whole cell.\n",
    "\n",
    "MODE = \"first\"   # \"first\"  -> first_pass_collect_links\n",
    "                 # \"second\" -> second_pass_extend_links\n",
    "\n",
    "if MODE == \"first\":\n",
    "    first_pass_collect_links()\n",
    "elif MODE == \"second\":\n",
    "    # start_page_override=None  -> continue from where you stopped\n",
    "    # start_page_override=0     -> force start from page 0 for all queries\n",
    "    second_pass_extend_links(start_page_override=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ef90f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "2_parse_resume_details.py\n",
    "\n",
    "Скрипт для парсинга детальной информации из резюме hh.ru\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "LINKS_FILE = \"resumes_links.csv\"\n",
    "DETAILS_FILE = \"resumes_details.csv\"\n",
    "PROCESSED_FILE = \"processed_ids.json\"\n",
    "NEW_BLOCKS_FILE = \"new_blocks_detected.csv\"\n",
    "\n",
    "PAUSE_BETWEEN_REQUESTS = 1.1\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/117.0.0.0 Safari/537.36\"\n",
    "    ),\n",
    "    \"Accept-Language\": \"ru-RU,ru;q=0.9\"\n",
    "}\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def ensure_dir_for_file(path):\n",
    "    d = os.path.dirname(os.path.abspath(path))\n",
    "    if d and not os.path.exists(d):\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "\n",
    "def load_processed_ids():\n",
    "    if os.path.exists(PROCESSED_FILE):\n",
    "        with open(PROCESSED_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                return set(json.load(f))\n",
    "            except Exception:\n",
    "                return set()\n",
    "    return set()\n",
    "\n",
    "def save_processed_ids(processed_set):\n",
    "    tmp = PROCESSED_FILE + \".tmp\"\n",
    "    with open(tmp, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(sorted(list(processed_set)), f, ensure_ascii=False, indent=2)\n",
    "    os.replace(tmp, PROCESSED_FILE)\n",
    "\n",
    "def append_row_to_csv(file_path, row, header):\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    ensure_dir_for_file(file_path)\n",
    "    with open(file_path, \"a\", encoding=\"utf-8-sig\", newline=\"\") as fh:\n",
    "        writer = csv.DictWriter(fh, fieldnames=header)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(row)\n",
    "\n",
    "def resume_id_from_url(url):\n",
    "    if not url:\n",
    "        return None\n",
    "    if \"/resume/\" in url:\n",
    "        return url.split(\"/resume/\")[1].split(\"?\")[0].strip(\"/\")\n",
    "    return None\n",
    "\n",
    "# ---------------- Parse resume detail ----------------\n",
    "def parse_resume_detail(url, known_blocks=None):\n",
    "    if known_blocks is None:\n",
    "        known_blocks = set()\n",
    "\n",
    "    try:\n",
    "        resp = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        html = resp.text\n",
    "    except Exception as e:\n",
    "        print(f\"    [detail] request failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    def safe_text(el):\n",
    "        return el.get_text(\" \", strip=True) if el else None\n",
    "\n",
    "    parsed_data = {\n",
    "        \"url\": url,\n",
    "        \"parsed_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    }\n",
    "\n",
    "    # Общие поля\n",
    "    parsed_data[\"title\"] = safe_text(\n",
    "        soup.select_one('span[data-qa=\"resume-block-title-position\"]') or \n",
    "        soup.select_one('h1') or \n",
    "        soup.select_one('h2')\n",
    "    )\n",
    "    parsed_data[\"location\"] = safe_text(soup.select_one('span[data-qa=\"resume-personal-address\"]'))\n",
    "    parsed_data[\"age\"] = safe_text(soup.select_one('span[data-qa=\"resume-personal-age\"]'))\n",
    "    parsed_data[\"gender\"] = safe_text(soup.select_one('span[data-qa=\"resume-personal-gender\"]'))\n",
    "    parsed_data[\"salary\"] = safe_text(soup.select_one('span[data-qa=\"resume-block-salary\"]'))\n",
    "\n",
    "    # Навыки\n",
    "    skills = [safe_text(t) for t in soup.select('span[data-qa=\"bloko-tag__text\"]')]\n",
    "    parsed_data[\"skills\"] = \", \".join(skills) if skills else None\n",
    "    parsed_data[\"skills_count\"] = len(skills)\n",
    "    \n",
    "    # О себе\n",
    "    about_el = (\n",
    "        soup.select_one('div[data-qa=\"resume-block-skills-content\"]') or \n",
    "        soup.select_one('div[data-qa=\"resume-block-summary\"]')\n",
    "    )\n",
    "    parsed_data[\"about\"] = safe_text(about_el)\n",
    "\n",
    "    # Опыт работы\n",
    "    experiences = []\n",
    "    exp_section = soup.select_one('div[data-qa=\"resume-block-experience\"]')\n",
    "    \n",
    "    if exp_section:\n",
    "        exp_items = exp_section.select('div.resume-block-item-gap')\n",
    "        \n",
    "        if exp_items:\n",
    "            print(f\"    [experience] Found {len(exp_items)} experience items\")\n",
    "            for exp in exp_items:\n",
    "                comp_el = (\n",
    "                    exp.select_one('div[data-qa=\"resume-block-experience-company\"]') or\n",
    "                    exp.select_one('div.bloko-text_strong')\n",
    "                )\n",
    "                company = safe_text(comp_el)\n",
    "                \n",
    "                pos_el = exp.select_one('div[data-qa=\"resume-block-experience-position\"]')\n",
    "                position = safe_text(pos_el)\n",
    "                \n",
    "                period_el = (\n",
    "                    exp.select_one('div.resume-block-item-gap-date') or\n",
    "                    exp.select_one('div.bloko-column_s-2')\n",
    "                )\n",
    "                period = safe_text(period_el)\n",
    "                \n",
    "                desc_el = exp.select_one('div[data-qa=\"resume-block-experience-description\"]')\n",
    "                description = safe_text(desc_el)\n",
    "                \n",
    "                experiences.append({\n",
    "                    \"company\": company,\n",
    "                    \"position\": position,\n",
    "                    \"period\": period,\n",
    "                    \"description\": description\n",
    "                })\n",
    "        else:\n",
    "            exp_items = exp_section.select('div.bloko-gap.bloko-gap_bottom')\n",
    "            \n",
    "            if not exp_items:\n",
    "                exp_items = exp_section.find_all('div', attrs={'data-qa': lambda x: x and 'experience' in x}, recursive=False)\n",
    "            \n",
    "            if exp_items:\n",
    "                print(f\"    [experience] Found {len(exp_items)} experience items (alternative)\")\n",
    "                for exp in exp_items:\n",
    "                    comp_el = (\n",
    "                        exp.select_one('[data-qa=\"resume-block-experience-company\"]') or\n",
    "                        exp.select_one('[data-qa=\"resume-block-experience-organization\"]') or\n",
    "                        exp.find('div', class_='bloko-text_strong')\n",
    "                    )\n",
    "                    company = safe_text(comp_el)\n",
    "                    \n",
    "                    pos_el = exp.select_one('[data-qa=\"resume-block-experience-position\"]')\n",
    "                    position = safe_text(pos_el)\n",
    "                    \n",
    "                    period_el = (\n",
    "                        exp.select_one('div.resume-block-item-gap-date') or\n",
    "                        exp.select_one('div.bloko-column_s-2') or\n",
    "                        exp.select_one('[data-qa*=\"period\"]')\n",
    "                    )\n",
    "                    period = safe_text(period_el)\n",
    "                    \n",
    "                    desc_el = exp.select_one('[data-qa=\"resume-block-experience-description\"]')\n",
    "                    description = safe_text(desc_el)\n",
    "                    \n",
    "                    if company or position:\n",
    "                        experiences.append({\n",
    "                            \"company\": company,\n",
    "                            \"position\": position,\n",
    "                            \"period\": period,\n",
    "                            \"description\": description\n",
    "                        })\n",
    "    \n",
    "    if not experiences:\n",
    "        print(f\"    [!] No experience found\")\n",
    "    \n",
    "    parsed_data[\"experience_json\"] = json.dumps(experiences, ensure_ascii=False)\n",
    "    parsed_data[\"experience_count\"] = len(experiences)\n",
    "\n",
    "    #образование\n",
    "    education = []\n",
    "    education_level_general = None  # Общий уровень образования\n",
    "    \n",
    "    edu_section = soup.select_one('div[data-qa=\"resume-block-education\"]')\n",
    "    \n",
    "    if edu_section:\n",
    "        \n",
    "        # Вариант 1: ищем полный текст из wrapper (включает основной уровень + подуровень)\n",
    "        level_wrapper = edu_section.select_one('div.resume-block__title-text-wrapper')\n",
    "        if level_wrapper:\n",
    "            education_level_general = safe_text(level_wrapper)\n",
    "            print(f\"    [education] General level (full): {education_level_general}\")\n",
    "        \n",
    "        # Вариант 2: если wrapper не найден, ищем только основной текст\n",
    "        if not education_level_general:\n",
    "            level_header = edu_section.select_one('div.resume-block__title-text')\n",
    "            if level_header:\n",
    "                education_level_general = safe_text(level_header)\n",
    "                print(f\"    [education] General level (basic): {education_level_general}\")\n",
    "        \n",
    "        # Ищем конкретные учебные заведения\n",
    "        edu_items = edu_section.select('div.resume-block-item-gap')\n",
    "        \n",
    "        if edu_items:\n",
    "            print(f\"    [education] Found {len(edu_items)} education items\")\n",
    "            for edu in edu_items:\n",
    "                name_el = (\n",
    "                    edu.select_one('div[data-qa=\"resume-block-education-name\"]') or\n",
    "                    edu.select_one('div.bloko-text_strong')\n",
    "                )\n",
    "                name = safe_text(name_el)\n",
    "                \n",
    "                org_el = edu.select_one('div[data-qa=\"resume-block-education-organization\"]')\n",
    "                organization = safe_text(org_el)\n",
    "                \n",
    "                period_el = (\n",
    "                    edu.select_one('div.resume-block-item-gap-date') or\n",
    "                    edu.select_one('div.bloko-column_s-2') or\n",
    "                    edu.select_one('div[data-qa=\"resume-block-education-year\"]')\n",
    "                )\n",
    "                period = safe_text(period_el)\n",
    "                \n",
    "                level_el = (\n",
    "                    edu.select_one('span.resume-block__title-text_sub') or\n",
    "                    edu.select_one('div.bloko-text_secondary')\n",
    "                )\n",
    "                level = safe_text(level_el)\n",
    "                \n",
    "                result_el = edu.select_one('div[data-qa=\"resume-block-education-result\"]')\n",
    "                result = safe_text(result_el)\n",
    "                \n",
    "                education.append({\n",
    "                    \"name\": name,\n",
    "                    \"organization\": organization,\n",
    "                    \"period\": period,\n",
    "                    \"level\": level,\n",
    "                    \"result\": result\n",
    "                })\n",
    "        else:\n",
    "            edu_items = edu_section.select('div.bloko-gap')\n",
    "            \n",
    "            if not edu_items:\n",
    "                edu_items = edu_section.find_all('div', recursive=False)\n",
    "            \n",
    "            if edu_items:\n",
    "                print(f\"    [education] Found {len(edu_items)} education items (alternative)\")\n",
    "                for edu in edu_items:\n",
    "                    name_el = (\n",
    "                        edu.select_one('[data-qa=\"resume-block-education-name\"]') or\n",
    "                        edu.find('div', class_='bloko-text_strong')\n",
    "                    )\n",
    "                    name = safe_text(name_el)\n",
    "                    \n",
    "                    org_el = edu.select_one('[data-qa=\"resume-block-education-organization\"]')\n",
    "                    organization = safe_text(org_el)\n",
    "                    \n",
    "                    period_el = (\n",
    "                        edu.select_one('div.resume-block-item-gap-date') or\n",
    "                        edu.select_one('div.bloko-column_s-2') or\n",
    "                        edu.select_one('[data-qa=\"resume-block-education-year\"]')\n",
    "                    )\n",
    "                    period = safe_text(period_el)\n",
    "                    \n",
    "                    level_el = (\n",
    "                        edu.select_one('span.resume-block__title-text_sub') or\n",
    "                        edu.select_one('div.bloko-text_secondary')\n",
    "                    )\n",
    "                    level = safe_text(level_el)\n",
    "                    \n",
    "                    result_el = edu.select_one('[data-qa=\"resume-block-education-result\"]')\n",
    "                    result = safe_text(result_el)\n",
    "                    \n",
    "                    if name:\n",
    "                        education.append({\n",
    "                            \"name\": name,\n",
    "                            \"organization\": organization,\n",
    "                            \"period\": period,\n",
    "                            \"level\": level,\n",
    "                            \"result\": result\n",
    "                        })\n",
    "    \n",
    "    if not education:\n",
    "        print(f\"    [!] No education found\")\n",
    "    \n",
    "    parsed_data[\"education_json\"] = json.dumps(education, ensure_ascii=False)\n",
    "    parsed_data[\"education_count\"] = len(education)\n",
    "    parsed_data[\"education_level\"] = education_level_general  # Добавляем отдельное поле\n",
    "\n",
    "    # ---------------- ADDITIONAL EDUCATION / COURSES ----------------\n",
    "    additional_education = []\n",
    "    \n",
    "    # Ищем блок с дополнительным образованием/курсами\n",
    "    additional_sections = soup.select('div[data-qa=\"resume-block-additional-education\"]')\n",
    "    \n",
    "    for section in additional_sections:\n",
    "        # Находим все курсы/сертификаты внутри секции\n",
    "        course_items = section.select('div.resume-block-item-gap')\n",
    "        \n",
    "        if not course_items:\n",
    "            course_items = section.select('div.bloko-gap')\n",
    "        \n",
    "        for course in course_items:\n",
    "            # Название курса/организации\n",
    "            name_el = (\n",
    "                course.select_one('div[data-qa=\"resume-block-education-name\"]') or\n",
    "                course.select_one('div.bloko-text_strong') or\n",
    "                course.select_one('a')\n",
    "            )\n",
    "            name = safe_text(name_el)\n",
    "            \n",
    "            # Организация, проводившая курс\n",
    "            org_el = course.select_one('div[data-qa=\"resume-block-education-organization\"]')\n",
    "            organization = safe_text(org_el)\n",
    "            \n",
    "            # Год/период прохождения\n",
    "            period_el = (\n",
    "                course.select_one('div.resume-block-item-gap-date') or\n",
    "                course.select_one('div.bloko-column_s-2') or\n",
    "                course.select_one('div[data-qa=\"resume-block-education-year\"]')\n",
    "            )\n",
    "            period = safe_text(period_el)\n",
    "            \n",
    "            # Описание/результат\n",
    "            result_el = course.select_one('div[data-qa=\"resume-block-education-result\"]')\n",
    "            result = safe_text(result_el)\n",
    "            \n",
    "            if name or organization:\n",
    "                additional_education.append({\n",
    "                    \"name\": name,\n",
    "                    \"organization\": organization,\n",
    "                    \"period\": period,\n",
    "                    \"result\": result\n",
    "                })\n",
    "    \n",
    "    if additional_education:\n",
    "        print(f\"    [courses] Found {len(additional_education)} additional education/courses\")\n",
    "    \n",
    "    parsed_data[\"additional_education_json\"] = json.dumps(additional_education, ensure_ascii=False)\n",
    "    parsed_data[\"additional_education_count\"] = len(additional_education)\n",
    "\n",
    "    # другие блоки-\n",
    "    new_blocks = []\n",
    "    blocks = soup.select('div[data-qa^=\"resume-block-\"]')\n",
    "    for block in blocks:\n",
    "        block_type = block.get(\"data-qa\", \"unknown_block\")\n",
    "        if block_type not in known_blocks:\n",
    "            new_blocks.append(block_type)\n",
    "            append_row_to_csv(\n",
    "                NEW_BLOCKS_FILE, \n",
    "                {\"url\": url, \"new_block\": block_type}, \n",
    "                header=[\"url\", \"new_block\"]\n",
    "            )\n",
    "            known_blocks.add(block_type)\n",
    "\n",
    "    return parsed_data\n",
    "\n",
    "def main():\n",
    "\n",
    "    processed = load_processed_ids()\n",
    "\n",
    "    if not os.path.exists(LINKS_FILE):\n",
    "        print(f\"[!] Links file not found: {LINKS_FILE}\")\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(LINKS_FILE, dtype=str)\n",
    "    rows = df.to_dict(orient=\"records\")\n",
    "    \n",
    "    # Фильтруем уже обработанные\n",
    "    rows_to_process = [r for r in rows if r.get(\"resume_id\") not in processed]\n",
    "    print(f\">> Need to process: {len(rows_to_process)} resumes\")\n",
    "\n",
    "    details_header = [\n",
    "        \"resume_id\", \"url\", \"title\", \"location\", \"age\", \"gender\", \"salary\",\n",
    "        \"skills\", \"skills_count\", \"about\",\n",
    "        \"experience_json\", \"experience_count\",\n",
    "        \"education_json\", \"education_count\", \"education_level\",\n",
    "        \"additional_education_json\", \"additional_education_count\",\n",
    "        \"parsed_at\"\n",
    "    ]\n",
    "\n",
    "    known_blocks = set()\n",
    "    success_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    for idx, r in enumerate(rows_to_process, start=1):\n",
    "        url = r.get(\"url\")\n",
    "        if not url:\n",
    "            continue\n",
    "        \n",
    "        rid = r.get(\"resume_id\") or resume_id_from_url(url)\n",
    "        if not rid:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n[{idx}/{len(rows_to_process)}] Parsing {rid}\")\n",
    "        print(f\"  URL: {url}\")\n",
    "        \n",
    "        detail = parse_resume_detail(url, known_blocks=known_blocks)\n",
    "        \n",
    "        if not detail:\n",
    "            print(\"  ✗ Could not parse detail\")\n",
    "            error_count += 1\n",
    "            time.sleep(PAUSE_BETWEEN_REQUESTS)\n",
    "            continue\n",
    "\n",
    "        row = {\"resume_id\": rid}\n",
    "        detail_to_save = detail.copy()\n",
    "        \n",
    "        row.update(detail_to_save)\n",
    "        append_row_to_csv(DETAILS_FILE, row, header=details_header)\n",
    "\n",
    "        processed.add(rid)\n",
    "        save_processed_ids(processed)\n",
    "        success_count += 1\n",
    "        time.sleep(PAUSE_BETWEEN_REQUESTS)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\">> Successfully parsed: {success_count}\")\n",
    "    print(f\">> Errors: {error_count}\")\n",
    "    print(f\">> Total processed: {len(processed)}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
