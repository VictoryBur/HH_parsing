# Парсинг и предобработка резюме с hh.ru

Два ноутбука реализуют полный пайплайн: от сбора резюме с сайта до готового к моделированию датасета.

---

## Файлы проекта

| Файл | Описание функционала |
|---|---|
| `HH_pars.ipynb` | Парсер: собирает ссылки на резюме и парсит их содержимое с hh.ru |
| `preprocess.ipynb` | Предобработка: очищает и трансформирует сырые данные в признаки для ML |

Поток данных:

```
hh.ru  →  HH_pars.ipynb  →  resumes_details.csv  →  preprocess.ipynb  →  итоговые CSV
```

---

## 1. HH_pars.ipynb — Сбор данных

### Что делает

Собирает резюме с hh.ru в два этапа: сначала собирает список ссылок, потом парсит каждое резюме по ссылке. Всё сохраняется инкрементально — если скрипт оборван на полпути, он продолжит с того места, где остановился.

### Этап 1 — Сбор ссылок (`first_pass_collect_links`)

Скрипт перебирает список из ~90 поисковых запросов (python разработчик, data scientist, devops engineer и т.д.), для каждого запроса пролистывает страницы выдачи hh.ru и собирает URL резюме.

Ключевые механизмы:

- **Дедупликация.** Все уже собранные `resume_id` загружаются в память из `resumes_links.csv`. Новая ссылка добавляется только если её `resume_id` ещё не встречается.
- **Умная остановка.** Если на N страницах подряд (по умолчанию 5) не появилось ни одного нового резюме, скрипт прекращает пагинацию для этого запроса и переходит к следующему.
- **Резервирование.** Каждая собранная ссылка сразу аппендится в `resumes_links.csv` — при крэше данные не теряются.

Второй режим — `second_pass_extend_links` — продолжает сбор по тем же запросам с той страницы, на которой в прошлый раз остановилось. Количество пройденных страниц оценивается по количеству уже собранных резюме для данного запроса.

Управление режимом через переменную `MODE`:
```python
MODE = "first"   # первичный сбор с 0-й страницы
MODE = "second"  # продолжение с последней собранной страницы
```

### Этап 2 — Парсинг резюме (`parse_resume_detail`)

Для каждой ссылки из `resumes_links.csv` скрипт загружает HTML страницы резюме и извлекает:

| Поле | Откуда берётся |
|---|---|
| `title` | Заголовок резюме (желаемая должность) |
| `location` | Город |
| `age` | Возраст (в виде текста, например «31 год») |
| `gender` | Пол |
| `salary` | Зарплата (текст с валютой и диапазонами) |
| `skills` | Навыки — строка через запятую |
| `about` | Блок «О себе» |
| `experience_json` | Полная история работы — JSON-массив объектов с полями: `company`, `position`, `period`, `description` |
| `education_json` | Образование — JSON-массив: `name` (вуз), `organization` (факультет), `period` (год) |
| `education_level` | Уровень образования текстом (например «Высшее образование (Магистр)») |
| `additional_education_json` | Курсы, сертификаты — аналогичный JSON |

Обработка идёт инкрементально: список уже обработанных `resume_id` хранится в `processed_ids.json`. Уже распарсенные резюме пропускаются. Результат — `resumes_details.csv`.

### Итоговые файлы

| Файл | Содержимое |
|---|---|
| `resumes_links.csv` | Список ссылок: resume_id, title, url, search_query, дата сбора |
| `resumes_details.csv` | Полный датасет с распарсенными резюме (26 053 строк, 18 столбцов) |
| `processed_ids.json` | Список уже обработанных resume_id (для возобновления) |

---

Бэкенд на hh.ru написан на Java, фронтенд использует React. Но несмотря на React, сайт работает с **серверным рендеринг (SSR)**: полный HTML со всем содержимым формируется на сервере и уже готовым отправляется клиенту. React на клиентской стороне только оживляет интерфейс — подключает обработчики событий, навигацию и т.д., — но не генерирует основной контент страницы, то есть когда мы делаем обычный `requests.get()`, в ответе приходит полная HTML-страница со всеми данными резюме. Парсинг делается через BeautifulSoup по статичному HTML.  Selenium, Playwright и другие инструменты для выполнения JavaScript не нужны.

hh.ru последовательно использует `data-qa` атрибуты для разметки HTML-блоков. Это маркеры для QA-тестирования сайта, которые hh.ru добавляет в свои шаблоны (это видно и в их опенсорс-репозиториях на GitHub). Они устойчивы к обновлениям дизайна: CSS-классы могут меняться при ребрендинге, а `data-qa` нет. Парсер целиком опирается на них:

```python
soup.select_one('span[data-qa="resume-block-title-position"]')   # должность
soup.select_one('span[data-qa="resume-personal-address"]')       # город
soup.select('span[data-qa="bloko-tag__text"]')                   # навыки (все тэги)
soup.select_one('div[data-qa="resume-block-experience"]')        # блок опыта
soup.select_one('div[data-qa="resume-block-education"]')         # блок образования
```

Внутри крупных блоков (experience, education) есть вложенная структура: каждый отдельный запись опыта или образования — это `div.resume-block-item-gap`, внутри которого снова `data-qa`-селекторы для компании, позиции, периода и описания:

```python
# Внутри одного элемента опыта:
exp.select_one('div[data-qa="resume-block-experience-company"]')
exp.select_one('div[data-qa="resume-block-experience-position"]')
exp.select_one('div[data-qa="resume-block-experience-description"]')
```

hh.ru периодически обновляет разметку. Если основной `data-qa`-селектор не находит элемент, парсер пробует альтернативы — по CSS-классам или по структуре DOM:

```python
# Пример для компании в опыте — три уровня fallback:
comp_el = (
    exp.select_one('div[data-qa="resume-block-experience-company"]') or
    exp.select_one('div[data-qa="resume-block-experience-organization"]') or
    exp.find('div', class_='bloko-text_strong')
)
```

Аналогичная схема для периодов, названий вузов и других полей. Это снижает риск полной поломки парсера при обновлении сайта.

Парсер также параллельно сканирует все блоки с атрибутами `data-qa`, начинающимися на `resume-block-`. Те, которые ещё не обрабатываются скрипт, логируются в `new_blocks_detected.csv`, что позволяет замечать появление новых секций резюме после обновления сайта и добавлять их в парсер.


## 2. preprocess.ipynb — Предобработка

### 2.1 Базовая очистка

Пропуски заполняются модой или нулями в зависимости от типа колонки. Удаляются технические поля (`resume_id`, `parsed_at`). Пол кодируется в 0/1. Возраст и зарплата извлекаются из текста регулярными выражениями, зарплата приводится к рублям.

### 2.2 Локации

Многие резюме заполнены на английском — города дублируются: «Moscow» и «Москва». Для каждого резюме определяется язык по полю `about` (библиотека `langdetect`). Если язык не русский — название города переводится через `googletrans`. Результат: все локации приводятся к русскому написанию.

### 2.3 Опыт работы

JSON-массив разворачивается в таблицу: последние 4 места работы — это столбцы `job_i_position`, `job_i_company`, `job_i_period`, `job_i_description`. Текстовые периоды парсятся в месяцы (`job_i_duration_months`), определяется флаг текущей работы (`job_i_is_current`). Парсер периодов понимает русский и английский.

Определяется флаг `is_top_it_company_flag`: предопределённый список IT-компаний (Яндекс, VK, Сбер, Тинькофф, Авито, Озон, EPAM и другие) сопоставляется с названиями из резюме после нормализации (нижний регистр, удаление ООО/LLC).

### 2.4 Образование

Уровень образования кодируется в порядковую шкалу 0–6 (от среднего до PhD). JSON образования разворачивается аналогично опыту: до 3 записей с полями вуза, специальности, года.

Каждому резюме присваивается рейтинг вуза (`best_university_rank`). Словарь содержит ~60 топовых университетов с номерами рангов (МФТИ = 2, ИТМО = 2.1 и т.д.). Сопоставление идёт по нормализованному тексту с поддержкой аббревиатур через `SequenceMatcher`. Если вуз не найден — значение 999.

### 2.5 Навыки

Исходное поле — строка навыков через запятую. Пайплайн:

1. Очистка и разбиение на токены.
2. Нормализация: словарь синонимов (`py` → `python`, `postgres` → `sql`), лемматизация русских слов через `pymorphy2`, удаление слов-паразитов.
3. Фаззи-мэтчинг через `rapidfuzz` (порог 85%) для опечаток и нестандартных написаний.
4. Отбор навыков, покрывающих 85% всех упоминаний → ~1775 навыков.
5. Бинарная матрица: для каждого резюме вектор 0/1 по отобранным навыкам.

Итог сохраняется в `normalized_skill_features.csv`

---

## 3. Итоговые файлы

| Файл | Содержимое |
|---|---|
| `resumes_links.csv` | Список собранных ссылок |
| `resumes_details.csv` | Сырой датасет после парсинга |
| `resumes_preprocessed.csv` | Датасет после очистки |
| `normalized_skill_features.csv` | Бинарная матрица навыков |

---

## 4. Зависимости

```
pandas          numpy           requests        beautifulsoup4
googletrans     langdetect      pymorphy2       rapidfuzz
tqdm            matplotlib      seaborn         scikit-learn
```
